module ValueIteration

using MDPs

export policy_evaluation, value_iteration, policy_iteration, PolicyEvaluationHook


"""
    bellman_backup_action_values_synchronous!(q::AbstractMatrix{T}, v::AbstractVector{T}, R::AbstractArray{T, 3}, Tr::AbstractArray{T, 3}, Œ≥::T)::T where T<:AbstractFloat

Perform a synchronous Bellman backup on the action values with formula:

q·µ¢‚Çä‚ÇÅ(a, s) <- Œ£‚Çõ‚Ä≤ Tr(s‚Ä≤, a, s) * (R(s‚Ä≤, a, s) + Œ≥ * v(s‚Ä≤))

# Arguments

- `q`: The action values to update, with indices [a, s].
- `v`: The value function, with indices [s].
- `R`: The reward function, with indices [s', a, s].
- `Tr`: The transition probability function, with indices [s', a, s].
- `Œ≥`: The discount factor.

# Returns

- `Œ¥`: The maximum change in the action values.
"""
function bellman_backup_action_values_synchronous!(q::AbstractMatrix{T}, v::AbstractVector{T}, R::AbstractArray{T, 3}, Tr::AbstractArray{T, 3}, Œ≥::T)::T where T<:AbstractFloat
    nactions::Int, nstates::Int = size(q)
    Œ¥::T = 0
    for s::Int in 1:nstates
        @inbounds for a::Int in 1:nactions
            q·µ¢‚Çä‚ÇÅ::T = 0
            @simd for s‚Ä≤ in 1:nstates
                q·µ¢‚Çä‚ÇÅ += Tr[s‚Ä≤, a, s]*(R[s‚Ä≤, a, s] + Œ≥ * v[s‚Ä≤])
            end
            Œ¥‚Çê‚Çõ::T = abs(q·µ¢‚Çä‚ÇÅ - q[a, s])
            if Œ¥‚Çê‚Çõ > Œ¥
                Œ¥ = Œ¥‚Çê‚Çõ
            end
            q[a, s] = q·µ¢‚Çä‚ÇÅ
        end
    end
    return Œ¥
end

function bellman_backup_action_values_vectorized(q::AbstractMatrix{T}, v::AbstractVector{T}, R::AbstractArray{T, 3}, Tr::AbstractArray{T, 3}, Œ≥::T)::Union{AbstractMatrix{T}, T} where T<:AbstractFloat
    tmp = @. Tr * (R + Œ≥ * v)
    q_new = @views sum(tmp, dims=1)[1, :, :]
    Œ¥ = maximum(abs.(q_new - q))
    return q_new, Œ¥
end

"""
    policy_evaluation(mdp::AbstractMDP{S, A}, œÄ::AbstractPolicy{_S, _A}, Œ≥::Real, horizon::Real; œµ=0.01)::Tuple{Float64, Vector{Float64}, Matrix{Float64}} where {S, A, _S, _A}

Perform policy evalution on the given MDP. The state and action spaces must be `IntegerSpace` or `EnumerableTensorSpace`.

# Arguments

- `mdp`: The MDP to perform value iteration on.
- `œÄ`: The policy to evaluate.
- `Œ≥`: The discount factor.
- `horizon`: The maximum number of iterations to perform.
- `œµ`: The convergence threshold.

# Returns

- `J`: The value over the start state distribution, under the given policy.
- `v`: The value function, where `v[s]` is the value of state `s` under the given policy.
- `q`: The Q function, where `q[a, s]` is the value of taking action `a` in state `s` under the given policy.
"""
function policy_evaluation(mdp::AbstractMDP{S, A}, œÄ::AbstractPolicy{_S, _A}, Œ≥::Real, horizon::Real; œµ=0.01)::Tuple{Float64, Vector{Float64}, Matrix{Float64}} where {S, A, _S, _A}
    nstates = length(state_space(mdp))
    nactions = length(action_space(mdp))
    ùïä = state_space(mdp)
    ùî∏ = action_space(mdp)
    _œÄ(s, a) = œÄ(_S == Int ? s : ùïä[s], _A == Int ? a : ùî∏[a])

    q::Matrix{Float64} = zeros(nactions, nstates)
    v::Vector{Float64} = zeros(nstates)
    T::Array{Float64, 3} = zeros(nstates, nactions, nstates)
    R::Array{Float64, 3} = zeros(nstates, nactions, nstates)
    œÄas = zeros(nactions, nstates)
    Threads.@threads for s::Int in eachindex(ùïä)
        for a::Int in eachindex(ùî∏)
            for s‚Ä≤::Int in eachindex(ùïä)
                @inbounds T[s‚Ä≤, a, s] = transition_probability(mdp, ùïä[s], ùî∏[a], ùïä[s‚Ä≤])
                @inbounds R[s‚Ä≤, a, s] = reward(mdp, ùïä[s], ùî∏[a], ùïä[s‚Ä≤])
            end
            œÄas = _œÄ(s, a)
        end
    end
    Œ≥f64 = Float64(Œ≥)

    i::int = 0
    while i < horizon
        Œ¥ = bellman_backup_action_values_synchronous!(q, v, R, T, Œ≥f64)
        @inbounds for s::Int in 1:nstates
            v[s] = 0
            for a::Int in 1:nactions  # why is @simd not working here? says "simd loop index must be a symbol"
                v[s] += œÄas[a, s] * q[a, s]
            end
        end
        i += 1
        Œ¥ < œµ && break
    end

    J = sum(start_state_distribution(mdp, ùïä) .* v)

    return J, v, q
end


"""
    value_iteration(mdp::AbstractMDP{S, A}, Œ≥::Real, horizon::Real; œµ=0.01, prealloc_q::Union{Matrix{Float64}, Nothing}=nothing, prealloc_v::Union{Vector{Float64}, Nothing}=nothing, prealloc_T::Union{Array{Float64, 3}, Nothing}=nothing, prealloc_R::Union{Array{Float64, 3}, Nothing}=nothing)::Tuple{Float64, Vector{Float64}, Matrix{Float64}} where {S, A}

Perform value iteration on the given MDP. The state and action spaces must be `IntegerSpace` or `EnumerableTensorSpace`. Preallocated arrays can be provided for the Q, V, T, and R functions. The indices for the Q function are [a, s], the indices for the T function are [s', a, s], and the indices for the R function are [s', a, s].

# Arguments

- `mdp`: The MDP to perform value iteration on.
- `Œ≥`: The discount factor.
- `horizon`: The maximum number of iterations to perform.
- `œµ`: The convergence threshold.
- `prealloc_v`: A preallocated vector to use for the v function.
- `prealloc_q`: A preallocated matrix to use for the q function, with indices [a, s].
- `prealloc_T`: A preallocated array to use for the transition probability function, with indices [s', a, s].
- `prealloc_R`: A preallocated array to use for the reward function, with indices [s', a, s].

# Returns

- `J`: The optimal value over the start state distribution.
- `v`: The optimal value function, where `v[s]` is the optimal value of state `s`.
- `q`: The optimal q function, where `q[a, s]` is the optimal value of taking action `a` in state `s`.
"""
function value_iteration(mdp::AbstractMDP{S, A}, Œ≥::Real, horizon::Real; œµ=0.01, prealloc_v::Union{Vector{Float64}, Nothing}=nothing, prealloc_q::Union{Matrix{Float64}, Nothing}=nothing, prealloc_T::Union{Array{Float64, 3}, Nothing}=nothing, prealloc_R::Union{Array{Float64, 3}, Nothing}=nothing)::Tuple{Float64, Vector{Float64}, Matrix{Float64}} where {S, A}
    nstates::Int = length(state_space(mdp))
    nactions::Int = length(action_space(mdp))
    ùïä::Union{IntegerSpace, EnumerableTensorSpace} = state_space(mdp)
    ùî∏::Union{IntegerSpace, EnumerableTensorSpace} = action_space(mdp)

    q::Matrix{Float64} = isnothing(prealloc_q) ? zeros(nactions, nstates) : prealloc_q
    @assert size(q) == (nactions, nstates)
    v::Vector{Float64} = isnothing(prealloc_v) ? zeros(nstates) : prealloc_v
    @assert length(v) == nstates
    Tr::Array{Float64, 3} = isnothing(prealloc_T) ? zeros(nstates, nactions, nstates) : prealloc_T
    @assert size(Tr) == (nstates, nactions, nstates)
    R::Array{Float64, 3} = isnothing(prealloc_R) ? zeros(nstates, nactions, nstates) : prealloc_R
    @assert size(R) == (nstates, nactions, nstates)
    Threads.@threads for s::Int in eachindex(ùïä)
        for a::Int in eachindex(ùî∏)
            for s‚Ä≤::Int in eachindex(ùïä)
                @inbounds Tr[s‚Ä≤, a, s] = transition_probability(mdp, ùïä[s], ùî∏[a], ùïä[s‚Ä≤])
                @inbounds R[s‚Ä≤, a, s] = reward(mdp, ùïä[s], ùî∏[a], ùïä[s‚Ä≤])
            end
        end
    end

    v, q = value_iteration(Tr, R, Œ≥, horizon; œµ=œµ, prealloc_v=v, prealloc_q=q)

    J::Float64 = sum(start_state_distribution(mdp, ùïä) .* v)

    return J, v, q
end


"""
    value_iteration(Tr::Array{T, 3}, R::Array{T, 3}, Œ≥::Real, horizon::Real; œµ=0.01, prealloc_v::Union{Vector{T}, Nothing}=nothing, prealloc_q::Union{Matrix{T}, Nothing}=nothing)::Tuple{Vector{T}, Matrix{T}} where T<:AbstractFloat

Perform value iteration given the transition probability and reward functions. The transition function indices are [s', a, s] and the reward function indices are [s', a, s]. Notice that s' is the first index in the transition function and the reward function. A preallocated vector and matrix can be provided for the V and Q functions. The indices for the Q function are [a, s].

# Arguments

- `Tr`: The transition probability function, where `Tr[s', a, s]` is the probability of transitioning to state `s'` given that action `a` is taken in state `s`.
- `R`: The reward function, where `R[s', a, s]` is the reward of transitioning to state `s'` given that action `a` is taken in state `s`.
- `Œ≥`: The discount factor.
- `horizon`: The maximum number of iterations to perform.
- `œµ`: The convergence threshold.
- `prealloc_v`: A preallocated vector to use for the v function.
- `prealloc_q`: A preallocated matrix to use for the q function.

# Returns

- `v`: The optimal value function, where `v[s]` is the optimal value of state `s`.
- `q`: The optimal Q function, where `q[a, s]` is the optimal value of taking action `a` in state `s`.
"""
function value_iteration(Tr::Array{T, 3}, R::Array{T, 3}, Œ≥::Real, horizon::Real; œµ=0.01, prealloc_v::Union{Vector{T}, Nothing}=nothing, prealloc_q::Union{Matrix{T}, Nothing}=nothing)::Tuple{Vector{T}, Matrix{T}} where T<:AbstractFloat
    nstates::Int = size(Tr, 1)
    nactions::Int = size(Tr, 2)

    q::Matrix{Float64} = isnothing(prealloc_q) ? zeros(nactions, nstates) : prealloc_q
    @assert size(q) == (nactions, nstates)
    v::Vector{Float64} = isnothing(prealloc_v) ? zeros(nstates) : prealloc_v
    @assert length(v) == nstates
    @assert size(Tr) == (nstates, nactions, nstates)
    @assert size(R) == (nstates, nactions, nstates)
    Œ≥f64 = Float64(Œ≥)

    i::Int = 0
    while i < horizon
        Œ¥ = bellman_backup_action_values_synchronous!(q, v, R, Tr, Œ≥f64)
        for s::Int in 1:nstates
            @inbounds v[s] = @views maximum(q[:, s])
        end
        i += 1
        Œ¥ < œµ && break
    end

    return v, q
end


"""
    policy_iteration(mdp::AbstractMDP{S, A}, Œ≥::Real, horizon::Real; œµ=0.01, policy_improvement_step = q -> GreedyPolicy(q), max_iterations=nothing)::Tuple{Vector{Float64}, Vector{Vector{Float64}}, Vector{Matrix{Float64}}, Vector{P}} where {S, A, P<:AbstractPolicy}

Perform policy iteration on the given MDP. The state and action spaces must be `IntegerSpace` or `EnumerableTensorSpace`.

# Arguments

- `mdp`: The MDP to perform value iteration on.
- `Œ≥`: The discount factor.
- `horizon`: The maximum number of iterations to perform.
- `œµ`: The convergence threshold.
- `policy_improvement_step`: The policy improvement step to use. Defaults to `q -> GreedyPolicy(q)`.
- `max_iterations`: The maximum number of iterations to perform. Defaults to `nothing`, which means that the algorithm will run until convergence.

# Returns

- `Js`: The optimal value over the start state distribution for each iteration.
- `vs`: The optimal value function for each iteration.
- `qs`: The optimal Q function for each iteration.
- `policies`: The policy for each iteration.
"""
function policy_iteration(mdp::AbstractMDP{S, A}, Œ≥::Real, horizon::Real; œµ=0.01, policy_improvement_step = q -> GreedyPolicy(q), max_iterations=nothing) where {S, A}
    Js::Vector{Float64} = Float64[]
    vs::Vector{Vector{Float64}} = Vector{Float64}[]
    qs::Vector{Matrix{Float64}} = Matrix{Float64}[]
    policies = []

    policy_changed = true
    policy = GreedyPolicy(zeros(length(action_space(mdp)), length(state_space(mdp))))
    states = state_space(mdp) |> collect
    actions = policy.(states)
    iters = 0
    while policy_changed
        J, v, q = policy_evaluation(mdp, policy, Œ≥, horizon; œµ=œµ)
        push!.((Js, vs, qs, policies), (J, v, q, policy))
        policy = policy_improvement_step(q)
        actions_after = policy.(states)
        # policy_changed = actions != actions_after
        actions = actions_after
        iters += 1
        if !isnothing(max_iterations) && iters >= max_iterations
            break
        end
    end
    
    return Js, vs, qs, policies
end

"""
    PolicyEvaluationHook(œÄ::AbstractPolicy, Œ≥::Real, horizon::Real, n::Int)

A hook for use with `MDPs.interact` that evaluates the given policy every `n` episodes and stores the result in `returns`.

# Arguments
- `œÄ`: The policy to evaluate.
- `Œ≥`: The discount factor.
- `horizon`: The horizon to use for policy evaluation.
- `n`: The number of episodes between evaluations.
"""
struct PolicyEvaluationHook <: AbstractHook
    œÄ::AbstractPolicy
    Œ≥::Real
    horizon::Real
    n::Int
    returns::Vector{Float64}
    PolicyEvaluationHook(œÄ::AbstractPolicy, Œ≥::Real, horizon::Real, n::Int) = new(œÄ, Œ≥, horizon, n, Float64[])
end

function MDPs.preexperiment(peh::PolicyEvaluationHook; env, kwargs...)
    push!(peh.returns, policy_evaluation(env, peh.œÄ, peh.Œ≥, peh.horizon)[1])
end

function MDPs.postepisode(peh::PolicyEvaluationHook; env, returns, kwargs...)
    if length(returns) % peh.n == 0
        push!(peh.returns, policy_evaluation(env, peh.œÄ, peh.Œ≥, peh.horizon)[1])
    end
end


end # module ValueIteration
