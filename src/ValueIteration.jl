module ValueIteration

using MDPs

export policy_evaluation, value_iteration, PolicyEvaluationHook

function bellman_backup_synchronous(mdp::AbstractMDP{Int, Int}, q::Matrix{Float64}, v::Vector{Float64}, Î³::Real)
    @inline R(s,a,sâ€²) = reward(mdp, s, a, sâ€²)
    @inline T(s,a,sâ€²) = transition_probability(mdp, s, a, sâ€²)
    nactions, nstates = size(q)
    Î´ = 0
    for s in 1:nstates
        for a in 1:nactions
            qáµ¢â‚Šâ‚ = sum(sâ€² -> T(s, a, sâ€²)*(R(s, a, sâ€²) + Î³ * v[sâ€²]), transition_support(mdp, s, a))
            Î´ = max(Î´, abs(qáµ¢â‚Šâ‚ - q[a, s]))
            q[a, s] = qáµ¢â‚Šâ‚
        end
    end
    return Î´
end

function bellman_backup_synchronous!(q::Matrix{Float64}, v::Vector{Float64}, R::Array{Float64, 3}, T::Array{Float64, 3}, Î³::Float64)::Float64
    nactions::Int, nstates::Int = size(q)
    Î´::Float64 = 0
    for s::Int in 1:nstates
        @inbounds for a::Int in 1:nactions
            qáµ¢â‚Šâ‚::Float64 = 0
            @simd for sâ€² in 1:nstates
                qáµ¢â‚Šâ‚ += T[sâ€², a, s]*(R[sâ€², a, s] + Î³ * v[sâ€²])
            end
            Î´â‚â‚›::Float64 = abs(qáµ¢â‚Šâ‚ - q[a, s])
            if Î´â‚â‚› > Î´
                Î´ = Î´â‚â‚›
            end
            q[a, s] = qáµ¢â‚Šâ‚
        end
    end
    return Î´
end

function policy_evaluation(mdp::AbstractMDP{Int, Int}, Ï€::AbstractPolicy{Int, Int}, Î³::Real, horizon::Real; Ïµ=0.01)::Tuple{Float64, Vector{Float64}, Matrix{Float64}}
    nstates = length(state_space(mdp))
    nactions = length(action_space(mdp))
    ğ•Š = state_space(mdp)
    ğ”¸ = action_space(mdp)

    q = zeros(nactions, nstates)
    v = zeros(nstates)

    i = 0
    while i < horizon
        Î´ = bellman_backup_synchronous(mdp, q, v, Î³)
        v .= map(s -> sum(a -> Ï€(s, a) * q[a, s], ğ”¸), ğ•Š)
        i += 1
        if Î´ < Ïµ
            # println("iter $i: breaking because Î´=$Î´ < Ïµ=$Ïµ")
            break
        end
    end

    J = sum(start_state_distribution(mdp, ğ•Š) .* v)

    return J, v, q
end


# function value_iteration(mdp::AbstractMDP{Int, Int}, Î³::Real, horizon::Real; Ïµ=0.01)::Tuple{Float64, Vector{Float64}, Matrix{Float64}}
#     nstates = length(state_space(mdp))
#     nactions = length(action_space(mdp))
#     ğ•Š = state_space(mdp)

#     q = zeros(nactions, nstates)
#     v = zeros(nstates)

#     # println("starting")
#     i = 0
#     while i < horizon
#         Î´ = bellman_backup_synchronous(mdp, q, v, Î³)
#         v .= transpose(maximum(q, dims=1))
#         i += 1
#         if Î´ < Ïµ
#             # println("iter $i: breaking because Î´=$Î´ < Ïµ=$Ïµ")
#             break
#         end
#     end

#     J = sum(start_state_distribution(mdp, ğ•Š) .* v)

#     return J, v, q
# end

"""
    value_iteration(mdp::AbstractMDP{Int, Int}, Î³::Real, horizon::Real; Ïµ=0.01, prealloc_q::Union{Matrix{Float64}, Nothing}=nothing, prealloc_v::Union{Vector{Float64}, Nothing}=nothing, prealloc_T::Union{Array{Float64, 3}, Nothing}=nothing, prealloc_R::Union{Array{Float64, 3}, Nothing}=nothing)::Tuple{Float64, Vector{Float64}, Matrix{Float64}}

Perform value iteration on the given MDP.

# Arguments

- `mdp`: The MDP to perform value iteration on.
- `Î³`: The discount factor.
- `horizon`: The maximum number of iterations to perform.
- `Ïµ`: The convergence threshold.
- `prealloc_q`: A preallocated matrix to use for the Q function.
- `prealloc_v`: A preallocated vector to use for the V function.
- `prealloc_T`: A preallocated array to use for the transition probability function.
- `prealloc_R`: A preallocated array to use for the reward function.

# Returns

- `J`: The optimal value over the start state distribution.
- `v`: The optimal value function.
- `q`: The optimal Q function.
"""
function value_iteration(mdp::AbstractMDP{Int, Int}, Î³::Real, horizon::Real; Ïµ=0.01, prealloc_q::Union{Matrix{Float64}, Nothing}=nothing, prealloc_v::Union{Vector{Float64}, Nothing}=nothing, prealloc_T::Union{Array{Float64, 3}, Nothing}=nothing, prealloc_R::Union{Array{Float64, 3}, Nothing}=nothing)::Tuple{Float64, Vector{Float64}, Matrix{Float64}}
    nstates::Int = length(state_space(mdp))
    nactions::Int = length(action_space(mdp))
    ğ•Š::IntegerSpace = state_space(mdp)

    q::Matrix{Float64} = isnothing(prealloc_q) ? zeros(nactions, nstates) : prealloc_q
    @assert size(q) == (nactions, nstates)
    v::Vector{Float64} = isnothing(prealloc_v) ? zeros(nstates) : prealloc_v
    @assert length(v) == nstates
    T::Array{Float64, 3} = isnothing(prealloc_T) ? zeros(nstates, nactions, nstates) : prealloc_T
    @assert size(T) == (nstates, nactions, nstates)
    R::Array{Float64, 3} = isnothing(prealloc_R) ? zeros(nstates, nactions, nstates) : prealloc_R
    @assert size(R) == (nstates, nactions, nstates)
    for s::Int in 1:nstates
        for a::Int in 1:nactions
            for sâ€²::Int in 1:nstates
                @inbounds T[sâ€², a, s] = transition_probability(mdp, s, a, sâ€²)
                @inbounds R[sâ€², a, s] = reward(mdp, s, a, sâ€²)
            end
        end
    end
    Î³f64 = Float64(Î³)

    i::Int = 0
    while i < horizon
        Î´ = bellman_backup_synchronous!(q, v, R, T, Î³f64)
        for s::Int in 1:nstates
            @inbounds v[s] = @views maximum(q[:, s])
        end
        i += 1
        Î´ < Ïµ && break
    end

    J::Float64 = sum(start_state_distribution(mdp, ğ•Š) .* v)

    return J, v, q
end


struct PolicyEvaluationHook <: AbstractHook
    Ï€::AbstractPolicy
    Î³::Real
    horizon::Real
    n::Int
    returns::Vector{Float64}
    PolicyEvaluationHook(Ï€::AbstractPolicy, Î³::Real, horizon::Real, n::Int) = new(Ï€, Î³, horizon, n, Float64[])
end

function MDPs.preexperiment(peh::PolicyEvaluationHook; env, kwargs...)
    push!(peh.returns, policy_evaluation(env, peh.Ï€, peh.Î³, peh.horizon)[1])
end

function MDPs.postepisode(peh::PolicyEvaluationHook; env, returns, kwargs...)
    if length(returns) % peh.n == 0
        push!(peh.returns, policy_evaluation(env, peh.Ï€, peh.Î³, peh.horizon)[1])
    end
end


end # module ValueIteration
