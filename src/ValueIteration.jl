module ValueIteration

using MDPs

export policy_evaluation, value_iteration, policy_iteration, PolicyEvaluationHook

function bellman_backup_synchronous(mdp::AbstractMDP{S, A}, q::Matrix{Float64}, v::Vector{Float64}, Î³::Real) where {S, A}
    @inline R(s,a,sâ€²) = reward(mdp, ğ•Š[s], ğ”¸[a], ğ•Š[sâ€²])
    @inline T(s,a,sâ€²) = transition_probability(mdp, ğ•Š[s], ğ”¸[a], ğ•Š[sâ€²])
    ğ•Š = state_space(mdp)
    ğ”¸ = action_space(mdp)
    Î´ = 0
    for s in eachindex(ğ•Š)
        for a in eachindex(ğ”¸)
            qáµ¢â‚Šâ‚ = sum(sâ€² -> T(s, a, sâ€²)*(R(s, a, sâ€²) + Î³ * v[sâ€²]), transition_support(mdp, ğ•Š[s], ğ”¸[a]))
            Î´ = max(Î´, abs(qáµ¢â‚Šâ‚ - q[a, s]))
            q[a, s] = qáµ¢â‚Šâ‚
        end
    end
    return Î´
end

function bellman_backup_synchronous!(q::Matrix{Float64}, v::Vector{Float64}, R::Array{Float64, 3}, T::Array{Float64, 3}, Î³::Float64)::Float64
    nactions::Int, nstates::Int = size(q)
    Î´::Float64 = 0
    for s::Int in 1:nstates
        @inbounds for a::Int in 1:nactions
            qáµ¢â‚Šâ‚::Float64 = 0
            @simd for sâ€² in 1:nstates
                qáµ¢â‚Šâ‚ += T[sâ€², a, s]*(R[sâ€², a, s] + Î³ * v[sâ€²])
            end
            Î´â‚â‚›::Float64 = abs(qáµ¢â‚Šâ‚ - q[a, s])
            if Î´â‚â‚› > Î´
                Î´ = Î´â‚â‚›
            end
            q[a, s] = qáµ¢â‚Šâ‚
        end
    end
    return Î´
end

"""
    policy_evaluation(mdp::AbstractMDP{S, A}, Ï€::AbstractPolicy{_S, _A}, Î³::Real, horizon::Real; Ïµ=0.01)::Tuple{Float64, Vector{Float64}, Matrix{Float64}} where {S, A, _S, _A}

Perform policy evalution on the given MDP. The state and action spaces must be `IntegerSpace` or `EnumerableTensorSpace`.

# Arguments

- `mdp`: The MDP to perform value iteration on.
- `Ï€`: The policy to evaluate.
- `Î³`: The discount factor.
- `horizon`: The maximum number of iterations to perform.
- `Ïµ`: The convergence threshold.

# Returns

- `J`: The optimal value over the start state distribution.
- `v`: The optimal value function.
- `q`: The optimal Q function.
"""
function policy_evaluation(mdp::AbstractMDP{S, A}, Ï€::AbstractPolicy{_S, _A}, Î³::Real, horizon::Real; Ïµ=0.01)::Tuple{Float64, Vector{Float64}, Matrix{Float64}} where {S, A, _S, _A}
    nstates = length(state_space(mdp))
    nactions = length(action_space(mdp))
    ğ•Š = state_space(mdp)
    ğ”¸ = action_space(mdp)

    q = zeros(nactions, nstates)
    v = zeros(nstates)

    _Ï€(s, a) = Ï€(_S == Int ? s : ğ•Š[s], _A == Int ? a : ğ”¸[a])

    i = 0
    while i < horizon
        Î´ = bellman_backup_synchronous(mdp, q, v, Î³)
        v .= map(s -> sum(a -> _Ï€(s, a) * q[a, s], eachindex(ğ”¸)), eachindex(ğ•Š))
        i += 1
        if Î´ < Ïµ
            break
        end
    end

    J = sum(start_state_distribution(mdp, ğ•Š) .* v)

    return J, v, q
end


"""
    value_iteration(mdp::AbstractMDP{S, A}, Î³::Real, horizon::Real; Ïµ=0.01, prealloc_q::Union{Matrix{Float64}, Nothing}=nothing, prealloc_v::Union{Vector{Float64}, Nothing}=nothing, prealloc_T::Union{Array{Float64, 3}, Nothing}=nothing, prealloc_R::Union{Array{Float64, 3}, Nothing}=nothing)::Tuple{Float64, Vector{Float64}, Matrix{Float64}} where {S, A}

Perform value iteration on the given MDP. The state and action spaces must be `IntegerSpace` or `EnumerableTensorSpace`.

# Arguments

- `mdp`: The MDP to perform value iteration on.
- `Î³`: The discount factor.
- `horizon`: The maximum number of iterations to perform.
- `Ïµ`: The convergence threshold.
- `prealloc_q`: A preallocated matrix to use for the Q function.
- `prealloc_v`: A preallocated vector to use for the V function.
- `prealloc_T`: A preallocated array to use for the transition probability function.
- `prealloc_R`: A preallocated array to use for the reward function.

# Returns

- `J`: The optimal value over the start state distribution.
- `v`: The optimal value function.
- `q`: The optimal Q function.
"""
function value_iteration(mdp::AbstractMDP{S, A}, Î³::Real, horizon::Real; Ïµ=0.01, prealloc_q::Union{Matrix{Float64}, Nothing}=nothing, prealloc_v::Union{Vector{Float64}, Nothing}=nothing, prealloc_T::Union{Array{Float64, 3}, Nothing}=nothing, prealloc_R::Union{Array{Float64, 3}, Nothing}=nothing)::Tuple{Float64, Vector{Float64}, Matrix{Float64}} where {S, A}
    nstates::Int = length(state_space(mdp))
    nactions::Int = length(action_space(mdp))
    ğ•Š::Union{IntegerSpace, EnumerableTensorSpace} = state_space(mdp)
    ğ”¸::Union{IntegerSpace, EnumerableTensorSpace} = action_space(mdp)

    q::Matrix{Float64} = isnothing(prealloc_q) ? zeros(nactions, nstates) : prealloc_q
    @assert size(q) == (nactions, nstates)
    v::Vector{Float64} = isnothing(prealloc_v) ? zeros(nstates) : prealloc_v
    @assert length(v) == nstates
    T::Array{Float64, 3} = isnothing(prealloc_T) ? zeros(nstates, nactions, nstates) : prealloc_T
    @assert size(T) == (nstates, nactions, nstates)
    R::Array{Float64, 3} = isnothing(prealloc_R) ? zeros(nstates, nactions, nstates) : prealloc_R
    @assert size(R) == (nstates, nactions, nstates)
    for s::Int in eachindex(ğ•Š)
        for a::Int in eachindex(ğ”¸)
            for sâ€²::Int in eachindex(ğ•Š)
                @inbounds T[sâ€², a, s] = transition_probability(mdp, ğ•Š[s], ğ”¸[a], ğ•Š[sâ€²])
                @inbounds R[sâ€², a, s] = reward(mdp, ğ•Š[s], ğ”¸[a], ğ•Š[sâ€²])
            end
        end
    end
    Î³f64 = Float64(Î³)

    i::Int = 0
    while i < horizon
        Î´ = bellman_backup_synchronous!(q, v, R, T, Î³f64)
        for s::Int in 1:nstates
            @inbounds v[s] = @views maximum(q[:, s])
        end
        i += 1
        Î´ < Ïµ && break
    end

    J::Float64 = sum(start_state_distribution(mdp, ğ•Š) .* v)

    return J, v, q
end


"""
    policy_iteration(mdp::AbstractMDP{S, A}, Î³::Real, horizon::Real; Ïµ=0.01, prealloc_q::Union{Matrix{Float64}, Nothing}=nothing, prealloc_v::Union{Vector{Float64}, Nothing}=nothing, prealloc_T::Union{Array{Float64, 3}, Nothing}=nothing, prealloc_R::Union{Array{Float64, 3}, Nothing}=nothing)::Tuple{Float64, Vector{Float64}, Matrix{Float64}} where {S, A}

Perform policy iteration on the given MDP. The state and action spaces must be `IntegerSpace` or `EnumerableTensorSpace`.

# Arguments

- `mdp`: The MDP to perform value iteration on.
- `Î³`: The discount factor.
- `horizon`: The maximum number of iterations to perform.
- `Ïµ`: The convergence threshold.
- `policy_improvement_step`: The policy improvement step to use. Defaults to `q -> GreedyPolicy(q)`.
- `max_iterations`: The maximum number of iterations to perform. Defaults to `nothing`, which means that the algorithm will run until convergence.

# Returns

- `Js`: The optimal value over the start state distribution for each iteration.
- `vs`: The optimal value function for each iteration.
- `qs`: The optimal Q function for each iteration.
- `policies`: The policy for each iteration.
"""
function policy_iteration(mdp::AbstractMDP{S, A}, Î³::Real, horizon::Real; Ïµ=0.01, policy_improvement_step = q -> GreedyPolicy(q), max_iterations=nothing) where {S, A}
    Js::Vector{Float64} = Float64[]
    vs::Vector{Vector{Float64}} = Vector{Float64}[]
    qs::Vector{Matrix{Float64}} = Matrix{Float64}[]
    policies = []

    policy_changed = true
    policy = GreedyPolicy(zeros(length(action_space(mdp)), length(state_space(mdp))))
    states = state_space(mdp) |> collect
    actions = policy.(states)
    iters = 0
    while policy_changed
        J, v, q = policy_evaluation(mdp, policy, Î³, horizon; Ïµ=Ïµ)
        push!.((Js, vs, qs, policies), (J, v, q, policy))
        policy = policy_improvement_step(q)
        actions_after = policy.(states)
        # policy_changed = actions != actions_after
        actions = actions_after
        iters += 1
        if !isnothing(max_iterations) && iters >= max_iterations
            break
        end
    end
    
    return Js, vs, qs, policies
end

"""
    PolicyEvaluationHook(Ï€::AbstractPolicy, Î³::Real, horizon::Real, n::Int)

A hook for use with `MDPs.interact` that evaluates the given policy every `n` episodes and stores the result in `returns`.

# Arguments
- `Ï€`: The policy to evaluate.
- `Î³`: The discount factor.
- `horizon`: The horizon to use for policy evaluation.
- `n`: The number of episodes between evaluations.
"""
struct PolicyEvaluationHook <: AbstractHook
    Ï€::AbstractPolicy
    Î³::Real
    horizon::Real
    n::Int
    returns::Vector{Float64}
    PolicyEvaluationHook(Ï€::AbstractPolicy, Î³::Real, horizon::Real, n::Int) = new(Ï€, Î³, horizon, n, Float64[])
end

function MDPs.preexperiment(peh::PolicyEvaluationHook; env, kwargs...)
    push!(peh.returns, policy_evaluation(env, peh.Ï€, peh.Î³, peh.horizon)[1])
end

function MDPs.postepisode(peh::PolicyEvaluationHook; env, returns, kwargs...)
    if length(returns) % peh.n == 0
        push!(peh.returns, policy_evaluation(env, peh.Ï€, peh.Î³, peh.horizon)[1])
    end
end


end # module ValueIteration
